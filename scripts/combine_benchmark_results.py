#!/usr/bin/env python3
"""
Map-Reduceçµæœã¨æ¨¡ç¯„è§£ç­”ã®æ¯”è¼ƒãƒ»çµåˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

benchmarkå®Ÿè¡Œçµæœã¨æ¨¡ç¯„è§£ç­”ï¼ˆExcelãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’çµåˆã—ã€
æ¯”è¼ƒå¯èƒ½ãªå½¢å¼ã§å‡ºåŠ›ã™ã‚‹
"""

import pandas as pd
import json
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import argparse

def load_reference_answers(excel_path: Path) -> Dict[int, str]:
    """æ¨¡ç¯„è§£ç­”ã‚’Excelã‹ã‚‰èª­ã¿è¾¼ã¿"""
    try:
        # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒ3è¡Œç›®ï¼ˆ0-indexed ã§ 2ï¼‰
        df = pd.read_excel(excel_path, header=2)
        
        # å®Ÿéš›ã®åˆ—åã«åˆã‚ã›ã¦èª¿æ•´
        question_col = 'è³ªå•'
        answer_col = 'å›ç­”'  # 'æ¨¡ç¯„è§£ç­”' ã§ã¯ãªã 'å›ç­”'
        process_col = 'éç¨‹ï¼ˆå¯¾å¿œæ€è€ƒã€å‚ç…§æ–‡çŒ®ã€å‚ç…§ç®‡æ‰€ï¼‰'
        
        if question_col not in df.columns or answer_col not in df.columns:
            available_cols = df.columns.tolist()
            print(f"âŒ å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", file=sys.stderr)
            print(f"   å¿…è¦: ['{question_col}', '{answer_col}']", file=sys.stderr)  
            print(f"   åˆ©ç”¨å¯èƒ½: {available_cols}", file=sys.stderr)
            return {}
        
        reference_answers = {}
        for idx, row in df.iterrows():
            if pd.notna(row[question_col]) and pd.notna(row[answer_col]):
                # è³ªå•ç•ªå·ã¯1-indexed
                reference_answers[idx + 1] = {
                    'question': str(row[question_col]).strip(),
                    'reference_answer': str(row[answer_col]).strip(),
                    'process': str(row[process_col]).strip() if pd.notna(row.get(process_col)) else ""
                }
        
        return reference_answers
        
    except Exception as e:
        print(f"âŒ Excelèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        return {}

def load_benchmark_result(run_dir: Path) -> Optional[Dict]:
    """benchmarkå®Ÿè¡Œçµæœã‚’èª­ã¿è¾¼ã¿"""
    try:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        metadata_path = run_dir / "metadata.json"
        if not metadata_path.exists():
            return None
            
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # çµ±åˆå›ç­”èª­ã¿è¾¼ã¿
        aggregate_path = run_dir / "aggregate_result.txt"
        aggregate_answer = ""
        if aggregate_path.exists():
            with open(aggregate_path, 'r', encoding='utf-8') as f:
                aggregate_answer = f.read()
        
        # Single QAçµæœèª­ã¿è¾¼ã¿ï¼ˆ.txtãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        single_qa_dir = run_dir / "single_qa"
        single_results = []
        if single_qa_dir.exists():
            for txt_file in sorted(single_qa_dir.glob("*.txt")):
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    single_results.append({
                        'filename': txt_file.name,
                        'content': content
                    })
        
        return {
            'metadata': metadata,
            'aggregate_answer': aggregate_answer,
            'single_results': single_results
        }
        
    except Exception as e:
        print(f"âŒ benchmarkçµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({run_dir}): {e}", file=sys.stderr)
        return None

def extract_question_number(run_id: str) -> Optional[int]:
    """run_idã‹ã‚‰è³ªå•ç•ªå·ã‚’æŠ½å‡ºï¼ˆä¾‹: "ç©ºãå®¶_Q1" -> 1ï¼‰"""
    try:
        if '_Q' in run_id:
            return int(run_id.split('_Q')[1])
        return None
    except:
        return None

def create_combined_excel(category: str, reference_answers: Dict[int, str], 
                         benchmark_results: Dict[str, Dict]) -> pd.DataFrame:
    """çµåˆãƒ‡ãƒ¼ã‚¿ã‚’Excelç”¨DataFrameã¨ã—ã¦ä½œæˆ"""
    
    rows = []
    
    # è³ªå•ã”ã¨ã«æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ä½œæˆ
    for q_num in sorted(reference_answers.keys()):
        ref_data = reference_answers[q_num]
        
        # å¯¾å¿œã™ã‚‹benchmarkçµæœã‚’æ¢ã™
        matching_run_id = None
        benchmark_data = None
        
        for run_id, result_data in benchmark_results.items():
            if extract_question_number(run_id) == q_num:
                matching_run_id = run_id
                benchmark_data = result_data
                break
        
        # 1è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        row = {
            'NO': q_num,
            'è³ªå•': ref_data['question'],
            'æ¨¡ç¯„è§£ç­”': ref_data['reference_answer'],
            'æ¨¡ç¯„è§£ç­”_éç¨‹': ref_data.get('process', ''),
        }
        
        if benchmark_data:
            metadata = benchmark_data['metadata']
            timing = metadata.get('results', {}).get('timing', {})
            
            # aggregate_answer ã®å†…å®¹å‡¦ç†
            aggregate_answer = benchmark_data['aggregate_answer']
            
            row.update({
                'ã‚·ã‚¹ãƒ†ãƒ å›ç­”': aggregate_answer,
                'å®Ÿè¡ŒID': matching_run_id,
                'å®Ÿè¡Œæ™‚é–“_ç§’': round(timing.get('total_time', 0), 1),
                'ãƒˆãƒ¼ã‚¯ãƒ³æ•°': metadata.get('results', {}).get('total_tokens', 0),
                'Single_QAå¹³å‡æ™‚é–“_ç§’': round(timing.get('single_qa_avg_time', 0), 1),
                'é›†ç´„æ™‚é–“_ç§’': round(timing.get('aggregate_time', 0), 1)
            })
        else:
            row.update({
                'ã‚·ã‚¹ãƒ†ãƒ å›ç­”': 'âŒ benchmarkçµæœãªã—',
                'å®Ÿè¡ŒID': '',
                'å®Ÿè¡Œæ™‚é–“_ç§’': 0,
                'ãƒˆãƒ¼ã‚¯ãƒ³æ•°': 0,
                'Single_QAå¹³å‡æ™‚é–“_ç§’': 0,
                'é›†ç´„æ™‚é–“_ç§’': 0
            })
        
        rows.append(row)
    
    return pd.DataFrame(rows)

def main():
    parser = argparse.ArgumentParser(description="benchmarkçµæœã¨æ¨¡ç¯„è§£ç­”ã‚’çµåˆ")
    parser.add_argument("--category", choices=["ç©ºãå®¶", "ç«‹åœ°é©æ­£åŒ–è¨ˆç”»"], 
                       help="å‡¦ç†ã™ã‚‹ã‚«ãƒ†ã‚´ãƒª")
    parser.add_argument("--output-dir", type=Path, default="evaluation",
                       help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: evaluation)")
    
    args = parser.parse_args()
    
    # QAãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    qa_dir = Path("data/QA")
    run_dir = Path("run")
    
    if not qa_dir.exists() or not run_dir.exists():
        print("âŒ data/QA ã¾ãŸã¯ run ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", file=sys.stderr)
        sys.exit(1)
    
    # ã‚«ãƒ†ã‚´ãƒªæŒ‡å®šãŒãªã„å ´åˆã¯å¯¾è©±çš„ã«é¸æŠ
    category = args.category
    if not category:
        categories = ["ç©ºãå®¶", "ç«‹åœ°é©æ­£åŒ–è¨ˆç”»"]
        print("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã—ã¦ãã ã•ã„:")
        for i, cat in enumerate(categories, 1):
            print(f"  {i}. {cat}")
        
        try:
            choice = int(input("ç•ªå·ã‚’å…¥åŠ›: ")) - 1
            category = categories[choice]
        except (ValueError, IndexError):
            print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™", file=sys.stderr)
            sys.exit(1)
    
    print(f"ğŸ“Š ã‚«ãƒ†ã‚´ãƒª: {category}")
    
    # æ¨¡ç¯„è§£ç­”èª­ã¿è¾¼ã¿
    excel_path = qa_dir / f"QA_{category}.xlsx"
    reference_answers = load_reference_answers(excel_path)
    
    if not reference_answers:
        print(f"âŒ {excel_path} ã‹ã‚‰æ¨¡ç¯„è§£ç­”ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ", file=sys.stderr)
        sys.exit(1)
    
    print(f"âœ… æ¨¡ç¯„è§£ç­”: {len(reference_answers)}ä»¶")
    
    # benchmarkçµæœèª­ã¿è¾¼ã¿
    benchmark_results = {}
    pattern = f"{category}_Q*"
    
    for run_path in run_dir.glob(pattern):
        if run_path.is_dir():
            result_data = load_benchmark_result(run_path)
            if result_data:
                benchmark_results[run_path.name] = result_data
    
    print(f"âœ… benchmarkçµæœ: {len(benchmark_results)}ä»¶")
    
    # çµåˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    combined_df = create_combined_excel(category, reference_answers, benchmark_results)
    
    # å‡ºåŠ›
    output_dir = args.output_dir
    output_dir.mkdir(exist_ok=True)
    
    output_path = output_dir / f"combined_{category}.xlsx"
    
    # CSVãŒä¸»åŠ›ã€Excelã¯å‚è€ƒç”¨ã¨ã—ã¦å‡ºåŠ›
    
    # ã¾ãšCSVå½¢å¼ã§å‡ºåŠ›ã—ã¦ãƒ†ã‚¹ãƒˆ
    csv_path = output_dir / f"combined_{category}.csv"
    combined_df.to_csv(csv_path, index=False, encoding='utf-8')
    print(f"âœ… CSVçµåˆãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›: {csv_path}")
    
    # Excelå½¢å¼ã§å‡ºåŠ› - xlsxwriterã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨
    try:
        with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
            combined_df.to_excel(writer, sheet_name='æ¯”è¼ƒçµæœ', index=False)
            
            # xlsxwriterã§ã®åˆ—å¹…èª¿æ•´
            worksheet = writer.sheets['æ¯”è¼ƒçµæœ']
            for idx, col in enumerate(combined_df.columns):
                # åˆ—ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€å¤§é•·ã‚’è¨ˆç®—
                max_len = max(combined_df[col].astype(str).map(len).max(), len(col))
                # é©åº¦ãªå¹…ã«èª¿æ•´ï¼ˆæœ€å¤§50æ–‡å­—ï¼‰
                worksheet.set_column(idx, idx, min(max_len + 2, 50))
    except Exception as e:
        print(f"âŒ Excelå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
    
    print(f"âœ… Excelçµåˆãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›: {output_path}")
    
    # è¦ç´„çµ±è¨ˆ
    matched_count = sum(1 for q_num in reference_answers.keys() 
                       if any(extract_question_number(run_id) == q_num 
                             for run_id in benchmark_results.keys()))
    
    print(f"\nğŸ“ˆ ãƒãƒƒãƒãƒ³ã‚°çµ±è¨ˆ:")
    print(f"   æ¨¡ç¯„è§£ç­”: {len(reference_answers)}ä»¶")
    print(f"   benchmark: {len(benchmark_results)}ä»¶")
    print(f"   ãƒãƒƒãƒ: {matched_count}ä»¶")

if __name__ == "__main__":
    main()